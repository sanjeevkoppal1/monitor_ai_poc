#!/usr/bin/env python3
"""
Crown Jewel Java Application Monitor - Proactive Issue Detection Agent
Advanced predictive monitoring agent that detects issues before they impact users.

This agent provides:
1. Predictive anomaly detection using machine learning
2. Trend analysis for capacity planning and performance prediction
3. Pattern recognition for emerging issues
4. Cross-correlation analysis between different data sources
5. Early warning system for potential failures
6. Baseline learning and adaptive thresholds
"""

import asyncio
import json
import time
import pickle
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from pathlib import Path
import tempfile
import statistics
from collections import defaultdict, deque
from enum import Enum

# Machine Learning imports
try:
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.cluster import DBSCAN
    from sklearn.decomposition import PCA
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    import pandas as pd
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# Statistical analysis
try:
    import scipy.stats as stats
    from scipy import signal
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

from ..core.agent_framework import (
    BaseAgent, Alert, AlertSeverity, MonitoringMetric, RemediationAction,
    AgentFactory
)

import structlog
logger = structlog.get_logger()


# =============================================================================
# PROACTIVE DETECTION DATA STRUCTURES
# =============================================================================

class PredictionConfidence(Enum):
    """Confidence levels for predictions."""
    HIGH = "high"           # >90% confidence
    MEDIUM = "medium"       # 70-90% confidence
    LOW = "low"            # 50-70% confidence
    UNCERTAIN = "uncertain" # <50% confidence


class IssueType(Enum):
    """Types of issues that can be predicted."""
    MEMORY_LEAK = "memory_leak"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    ERROR_RATE_INCREASE = "error_rate_increase"
    CAPACITY_LIMIT = "capacity_limit"
    DEPENDENCY_FAILURE = "dependency_failure"
    CONFIGURATION_DRIFT = "configuration_drift"
    SECURITY_ANOMALY = "security_anomaly"


@dataclass
class PredictiveAlert:
    """
    Represents a predictive alert generated by proactive analysis.
    Contains prediction details, confidence metrics, and recommended actions.
    """
    prediction_id: str                    # Unique prediction identifier
    issue_type: IssueType                # Type of predicted issue
    confidence: PredictionConfidence     # Confidence level of prediction
    predicted_time: datetime             # When issue is predicted to occur
    current_severity: AlertSeverity      # Current severity level
    predicted_severity: AlertSeverity    # Predicted severity at occurrence
    
    # Prediction details
    description: str                     # Human-readable description
    affected_components: List[str]       # Components likely to be affected
    root_cause_indicators: List[str]     # Metrics/patterns indicating issue
    
    # Evidence and metrics
    supporting_evidence: Dict[str, Any]  # Data supporting the prediction
    trend_analysis: Dict[str, float]     # Trend metrics and rates
    baseline_deviation: float            # How much current state deviates from baseline
    
    # Recommended actions
    preventive_actions: List[str]        # Actions to prevent the issue
    monitoring_actions: List[str]        # Additional monitoring to implement
    escalation_timeline: Dict[str, str]  # When to escalate based on timeline
    
    # Metadata
    model_used: str                      # ML model or algorithm used
    data_sources: List[str]              # Sources of data used for prediction
    last_updated: datetime = field(default_factory=datetime.utcnow)
    false_positive_probability: float = 0.0  # Estimated false positive rate


@dataclass
class TrendAnalysis:
    """
    Represents trend analysis for a specific metric or pattern.
    Used for predictive modeling and capacity planning.
    """
    metric_name: str                     # Name of the metric being analyzed
    time_series_data: List[Tuple[datetime, float]]  # Historical data points
    
    # Trend characteristics
    trend_direction: str                 # "increasing", "decreasing", "stable", "volatile"
    trend_strength: float                # Strength of trend (0.0-1.0)
    rate_of_change: float               # Rate of change per unit time
    acceleration: float                  # Rate of change of rate of change
    
    # Statistical metrics
    mean_value: float                    # Average value
    std_deviation: float                 # Standard deviation
    min_value: float                     # Minimum observed value
    max_value: float                     # Maximum observed value
    
    # Prediction metrics
    predicted_values: List[Tuple[datetime, float]]  # Future predicted values
    confidence_intervals: List[Tuple[float, float]]  # Confidence intervals for predictions
    time_to_threshold: Optional[timedelta] = None   # Time until threshold breach
    
    # Seasonality and patterns
    seasonal_patterns: Dict[str, Any] = field(default_factory=dict)
    anomaly_score: float = 0.0          # Current anomaly score
    baseline_established: bool = False   # Whether baseline is established


@dataclass
class BaselineMetrics:
    """
    Represents baseline performance metrics for normal operation.
    Used as reference for anomaly detection and trend analysis.
    """
    metric_name: str                     # Name of the metric
    establishment_period: timedelta     # Time period used to establish baseline
    
    # Statistical baselines
    mean: float                         # Baseline mean value
    median: float                       # Baseline median value
    std_dev: float                      # Baseline standard deviation
    percentiles: Dict[int, float]       # Percentile values (5, 25, 75, 95, 99)
    
    # Time-based patterns
    hourly_patterns: Dict[int, float]   # Average values by hour of day
    daily_patterns: Dict[int, float]    # Average values by day of week
    seasonal_factors: Dict[str, float]  # Seasonal adjustment factors
    
    # Adaptive learning
    learning_rate: float = 0.1          # Rate at which baseline adapts to new data
    confidence_level: float = 0.0       # Confidence in baseline accuracy
    last_updated: datetime = field(default_factory=datetime.utcnow)
    sample_count: int = 0               # Number of samples used in baseline


# =============================================================================
# PROACTIVE DETECTION AGENT IMPLEMENTATION
# =============================================================================

class ProactiveDetectionAgent(BaseAgent):
    """
    Advanced proactive issue detection agent that uses machine learning,
    statistical analysis, and pattern recognition to predict issues before
    they impact users.
    
    Core capabilities:
    - Predictive anomaly detection using isolation forests and clustering
    - Time series forecasting for capacity planning
    - Cross-correlation analysis between metrics
    - Adaptive baseline learning
    - Multi-model ensemble predictions
    """
    
    def __init__(self, agent_id: str, config: Dict[str, Any]):
        """
        Initialize the proactive detection agent with configuration.
        
        Args:
            agent_id: Unique identifier for this agent instance
            config: Configuration dictionary with detection parameters
        """
        super().__init__(agent_id, config)
        
        # Configuration
        self.prediction_window = config.get('prediction_window_hours', 2)
        self.baseline_period_days = config.get('baseline_period_days', 7)
        self.anomaly_threshold = config.get('anomaly_threshold', 0.7)
        self.min_confidence_threshold = config.get('min_confidence_threshold', 0.6)
        self.enable_ml_detection = config.get('enable_ml_detection', True) and ML_AVAILABLE
        
        # Data storage
        self.metrics_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.baselines: Dict[str, BaselineMetrics] = {}
        self.trend_analyses: Dict[str, TrendAnalysis] = {}
        self.active_predictions: Dict[str, PredictiveAlert] = {}
        
        # Machine learning models
        self.ml_models: Dict[str, Any] = {}
        self.scalers: Dict[str, Any] = {}
        
        # State tracking
        self.last_analysis_time: Optional[datetime] = None
        self.model_last_trained: Dict[str, datetime] = {}
        self.prediction_accuracy_history: List[Dict[str, Any]] = []
        
        logger.info("ProactiveDetectionAgent initialized", 
                   agent_id=self.agent_id,
                   ml_enabled=self.enable_ml_detection)
    
    async def initialize(self) -> bool:
        """
        Initialize the proactive detection agent.
        Load saved models and baselines, initialize ML components.
        
        Returns:
            True if initialization successful, False otherwise
        """
        try:
            logger.info("Initializing ProactiveDetectionAgent", agent_id=self.agent_id)
            
            # Create necessary directories
            self.data_dir = Path(self.config.get('data_directory', '/tmp/crown-jewel-proactive'))
            self.data_dir.mkdir(parents=True, exist_ok=True)
            
            # Load saved baselines and models
            await self._load_saved_data()
            
            # Initialize ML models if enabled
            if self.enable_ml_detection:
                await self._initialize_ml_models()
            
            # Validate configuration
            await self._validate_configuration()
            
            logger.info("ProactiveDetectionAgent initialized successfully", agent_id=self.agent_id)
            return True
            
        except Exception as e:
            logger.error("Failed to initialize ProactiveDetectionAgent",
                        agent_id=self.agent_id, error=str(e))
            return False
    
    async def execute(self) -> Dict[str, Any]:
        """
        Execute proactive detection analysis.
        
        Main execution flow:
        1. Collect current metrics from other agents
        2. Update baselines and trend analyses
        3. Perform anomaly detection
        4. Generate predictive alerts
        5. Update ML models
        
        Returns:
            Dictionary containing execution results and detected issues
        """
        start_time = time.time()
        results = {
            'predictions_generated': 0,
            'anomalies_detected': 0,
            'models_updated': 0,
            'baselines_updated': 0,
            'execution_time_ms': 0,
            'active_predictions': len(self.active_predictions)
        }
        
        try:
            logger.info("Starting proactive detection analysis", agent_id=self.agent_id)
            
            # Step 1: Collect current metrics
            current_metrics = await self._collect_current_metrics()
            if not current_metrics:
                logger.warning("No metrics available for analysis", agent_id=self.agent_id)
                return results
            
            # Step 2: Update historical data and baselines
            await self._update_metrics_history(current_metrics)
            baselines_updated = await self._update_baselines(current_metrics)
            results['baselines_updated'] = baselines_updated
            
            # Step 3: Perform trend analysis
            trend_updates = await self._perform_trend_analysis()
            
            # Step 4: Anomaly detection
            if self.enable_ml_detection:
                anomalies = await self._detect_ml_anomalies(current_metrics)
                results['anomalies_detected'] = len(anomalies)
            else:
                anomalies = await self._detect_statistical_anomalies(current_metrics)
                results['anomalies_detected'] = len(anomalies)
            
            # Step 5: Generate predictive alerts
            predictions = await self._generate_predictive_alerts(current_metrics, anomalies)
            results['predictions_generated'] = len(predictions)
            
            # Step 6: Update and retrain ML models
            if self.enable_ml_detection:
                models_updated = await self._update_ml_models()
                results['models_updated'] = models_updated
            
            # Step 7: Clean up old predictions and data
            await self._cleanup_old_data()
            
            # Step 8: Emit metrics and alerts
            await self._emit_results(predictions, anomalies, current_metrics)
            
            self.last_analysis_time = datetime.utcnow()
            
        except Exception as e:
            logger.error("Error during proactive detection execution",
                        agent_id=self.agent_id, error=str(e))
            await self.emit_alert(Alert(
                id=f"proactive_detection_error_{int(time.time())}",
                title="Proactive Detection Agent Error",
                description=f"Error during proactive detection: {str(e)}",
                severity=AlertSeverity.MEDIUM,
                source=self.agent_id,
                timestamp=datetime.utcnow(),
                tags=['proactive_detection', 'agent_error']
            ))
        
        finally:
            results['execution_time_ms'] = (time.time() - start_time) * 1000
            logger.info("Proactive detection analysis completed",
                       agent_id=self.agent_id, **results)
        
        return results
    
    async def cleanup(self) -> None:
        """
        Clean up resources and save current state.
        """
        try:
            logger.info("Cleaning up ProactiveDetectionAgent", agent_id=self.agent_id)
            
            # Save current state
            await self._save_current_data()
            
            # Clear memory structures
            self.metrics_history.clear()
            self.ml_models.clear()
            self.scalers.clear()
            
            logger.info("ProactiveDetectionAgent cleanup completed", agent_id=self.agent_id)
            
        except Exception as e:
            logger.error("Error during ProactiveDetectionAgent cleanup",
                        agent_id=self.agent_id, error=str(e))
    
    # =========================================================================
    # DATA COLLECTION AND MANAGEMENT METHODS
    # =========================================================================
    
    async def _collect_current_metrics(self) -> Dict[str, Any]:
        """
        Collect current metrics from all available sources.
        
        Returns:
            Dictionary of current metric values organized by source
        """
        metrics = {}
        
        try:
            # Get metrics from the orchestrator's global state
            if hasattr(self, 'orchestrator') and self.orchestrator:
                global_metrics = self.orchestrator.get_current_metrics()
                metrics.update(global_metrics)
            
            # Add timestamp
            metrics['collection_timestamp'] = datetime.utcnow()
            
            logger.debug("Collected current metrics", 
                        agent_id=self.agent_id, 
                        metric_count=len(metrics))
            
        except Exception as e:
            logger.error("Error collecting current metrics",
                        agent_id=self.agent_id, error=str(e))
        
        return metrics
    
    async def _update_metrics_history(self, current_metrics: Dict[str, Any]) -> None:
        """
        Update historical metrics storage with current values.
        
        Args:
            current_metrics: Current metric values to add to history
        """
        timestamp = current_metrics.get('collection_timestamp', datetime.utcnow())
        
        for metric_name, value in current_metrics.items():
            if metric_name == 'collection_timestamp':
                continue
            
            # Convert to float if possible
            try:
                numeric_value = float(value)
                self.metrics_history[metric_name].append((timestamp, numeric_value))
            except (ValueError, TypeError):
                # Skip non-numeric values
                continue
    
    async def _update_baselines(self, current_metrics: Dict[str, Any]) -> int:
        """
        Update baseline metrics for normal operation detection.
        
        Args:
            current_metrics: Current metric values
            
        Returns:
            Number of baselines updated
        """
        updated_count = 0
        
        for metric_name, value in current_metrics.items():
            if metric_name == 'collection_timestamp':
                continue
            
            try:
                numeric_value = float(value)
            except (ValueError, TypeError):
                continue
            
            # Get or create baseline
            if metric_name not in self.baselines:
                self.baselines[metric_name] = BaselineMetrics(
                    metric_name=metric_name,
                    establishment_period=timedelta(days=self.baseline_period_days),
                    mean=numeric_value,
                    median=numeric_value,
                    std_dev=0.0,
                    percentiles={}
                )
            
            baseline = self.baselines[metric_name]
            
            # Update baseline with adaptive learning
            if baseline.sample_count > 0:
                # Exponential moving average
                baseline.mean = (1 - baseline.learning_rate) * baseline.mean + \
                               baseline.learning_rate * numeric_value
            else:
                baseline.mean = numeric_value
            
            baseline.sample_count += 1
            baseline.last_updated = datetime.utcnow()
            
            # Recalculate full statistics periodically
            if baseline.sample_count % 100 == 0:
                await self._recalculate_baseline_statistics(metric_name)
            
            updated_count += 1
        
        return updated_count
    
    async def _recalculate_baseline_statistics(self, metric_name: str) -> None:
        """
        Recalculate full baseline statistics for a metric.
        
        Args:
            metric_name: Name of the metric to recalculate
        """
        if metric_name not in self.metrics_history:
            return
        
        # Get recent data for baseline calculation
        recent_data = list(self.metrics_history[metric_name])
        if len(recent_data) < 10:
            return
        
        # Extract values from (timestamp, value) tuples
        values = [value for timestamp, value in recent_data[-1000:]]  # Last 1000 points
        
        baseline = self.baselines[metric_name]
        
        # Calculate statistics
        baseline.mean = statistics.mean(values)
        baseline.median = statistics.median(values)
        baseline.std_dev = statistics.stdev(values) if len(values) > 1 else 0.0
        baseline.min_value = min(values)
        baseline.max_value = max(values)
        
        # Calculate percentiles
        if SCIPY_AVAILABLE:
            baseline.percentiles = {
                5: float(np.percentile(values, 5)),
                25: float(np.percentile(values, 25)),
                75: float(np.percentile(values, 75)),
                95: float(np.percentile(values, 95)),
                99: float(np.percentile(values, 99))
            }
        
        baseline.confidence_level = min(0.95, len(values) / 1000.0)
    
    # =========================================================================
    # TREND ANALYSIS METHODS
    # =========================================================================
    
    async def _perform_trend_analysis(self) -> int:
        """
        Perform trend analysis on all metrics with sufficient history.
        
        Returns:
            Number of trend analyses updated
        """
        updated_count = 0
        
        for metric_name, history in self.metrics_history.items():
            if len(history) < 20:  # Need minimum data points
                continue
            
            try:
                trend_analysis = await self._analyze_metric_trend(metric_name, history)
                if trend_analysis:
                    self.trend_analyses[metric_name] = trend_analysis
                    updated_count += 1
            except Exception as e:
                logger.error("Error analyzing trend for metric",
                           metric_name=metric_name, error=str(e))
        
        return updated_count
    
    async def _analyze_metric_trend(self, metric_name: str, 
                                   history: deque) -> Optional[TrendAnalysis]:
        """
        Analyze trend for a specific metric.
        
        Args:
            metric_name: Name of the metric
            history: Historical data points
            
        Returns:
            TrendAnalysis object or None if analysis fails
        """
        if len(history) < 20:
            return None
        
        # Extract data
        data_points = list(history)[-100:]  # Use last 100 points
        timestamps = [ts for ts, value in data_points]
        values = [value for ts, value in data_points]
        
        # Convert timestamps to numeric for analysis
        base_time = timestamps[0]
        time_numeric = [(ts - base_time).total_seconds() for ts in timestamps]
        
        # Basic statistics
        mean_value = statistics.mean(values)
        std_deviation = statistics.stdev(values) if len(values) > 1 else 0.0
        min_value = min(values)
        max_value = max(values)
        
        # Trend calculation
        if SCIPY_AVAILABLE and len(values) > 5:
            # Linear regression for trend
            slope, intercept, r_value, p_value, std_err = stats.linregress(time_numeric, values)
            
            # Determine trend characteristics
            trend_strength = abs(r_value) if not np.isnan(r_value) else 0.0
            rate_of_change = slope
            
            if trend_strength > 0.7:
                if slope > 0:
                    trend_direction = "increasing"
                else:
                    trend_direction = "decreasing"
            elif trend_strength > 0.3:
                trend_direction = "stable"
            else:
                trend_direction = "volatile"
            
            # Calculate acceleration (second derivative)
            if len(values) > 10:
                # Simple acceleration calculation
                mid_point = len(values) // 2
                first_half_slope = np.polyfit(time_numeric[:mid_point], values[:mid_point], 1)[0]
                second_half_slope = np.polyfit(time_numeric[mid_point:], values[mid_point:], 1)[0]
                acceleration = second_half_slope - first_half_slope
            else:
                acceleration = 0.0
        else:
            # Simple trend calculation without scipy
            if len(values) >= 3:
                recent_avg = statistics.mean(values[-3:])
                earlier_avg = statistics.mean(values[:3])
                
                if recent_avg > earlier_avg * 1.1:
                    trend_direction = "increasing"
                    trend_strength = 0.6
                elif recent_avg < earlier_avg * 0.9:
                    trend_direction = "decreasing"
                    trend_strength = 0.6
                else:
                    trend_direction = "stable"
                    trend_strength = 0.3
                
                rate_of_change = (recent_avg - earlier_avg) / len(values)
                acceleration = 0.0
            else:
                trend_direction = "stable"
                trend_strength = 0.0
                rate_of_change = 0.0
                acceleration = 0.0
        
        # Simple prediction (linear extrapolation)
        predicted_values = []
        confidence_intervals = []
        
        if trend_strength > 0.5 and rate_of_change != 0:
            # Predict next few points
            last_time = timestamps[-1]
            last_value = values[-1]
            
            for i in range(1, 6):  # Predict next 5 time periods
                future_time = last_time + timedelta(minutes=i * 5)  # Assuming 5-minute intervals
                predicted_value = last_value + (rate_of_change * i * 300)  # 300 seconds = 5 minutes
                
                predicted_values.append((future_time, predicted_value))
                
                # Simple confidence interval based on standard deviation
                confidence_interval = (
                    predicted_value - std_deviation,
                    predicted_value + std_deviation
                )
                confidence_intervals.append(confidence_interval)
        
        # Check if baseline exists for anomaly scoring
        anomaly_score = 0.0
        if metric_name in self.baselines:
            baseline = self.baselines[metric_name]
            if baseline.std_dev > 0:
                current_value = values[-1]
                z_score = abs(current_value - baseline.mean) / baseline.std_dev
                anomaly_score = min(1.0, z_score / 3.0)  # Normalize to 0-1
        
        return TrendAnalysis(
            metric_name=metric_name,
            time_series_data=data_points,
            trend_direction=trend_direction,
            trend_strength=trend_strength,
            rate_of_change=rate_of_change,
            acceleration=acceleration,
            mean_value=mean_value,
            std_deviation=std_deviation,
            min_value=min_value,
            max_value=max_value,
            predicted_values=predicted_values,
            confidence_intervals=confidence_intervals,
            anomaly_score=anomaly_score,
            baseline_established=metric_name in self.baselines
        )
    
    # =========================================================================
    # ANOMALY DETECTION METHODS
    # =========================================================================
    
    async def _detect_ml_anomalies(self, current_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect anomalies using machine learning models.
        
        Args:
            current_metrics: Current metric values
            
        Returns:
            List of detected anomalies
        """
        anomalies = []
        
        if not ML_AVAILABLE:
            return await self._detect_statistical_anomalies(current_metrics)
        
        try:
            # Prepare data for ML analysis
            feature_data = await self._prepare_ml_features()
            
            if len(feature_data) < 50:  # Need minimum data for ML
                return await self._detect_statistical_anomalies(current_metrics)
            
            # Use isolation forest for anomaly detection
            if 'isolation_forest' not in self.ml_models:
                self.ml_models['isolation_forest'] = IsolationForest(
                    contamination=0.1,  # Expect 10% anomalies
                    random_state=42,
                    n_estimators=100
                )
                self.ml_models['isolation_forest'].fit(feature_data)
            
            # Get current feature vector
            current_features = await self._extract_current_features(current_metrics)
            
            if current_features is not None:
                # Predict anomaly
                anomaly_score = self.ml_models['isolation_forest'].decision_function([current_features])[0]
                is_anomaly = self.ml_models['isolation_forest'].predict([current_features])[0] == -1
                
                if is_anomaly:
                    anomalies.append({
                        'type': 'ml_anomaly',
                        'score': float(anomaly_score),
                        'features': current_features.tolist() if hasattr(current_features, 'tolist') else current_features,
                        'detection_method': 'isolation_forest',
                        'timestamp': datetime.utcnow()
                    })
            
        except Exception as e:
            logger.error("Error in ML anomaly detection", 
                        agent_id=self.agent_id, error=str(e))
            # Fallback to statistical detection
            return await self._detect_statistical_anomalies(current_metrics)
        
        return anomalies
    
    async def _detect_statistical_anomalies(self, current_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Detect anomalies using statistical methods.
        
        Args:
            current_metrics: Current metric values
            
        Returns:
            List of detected anomalies
        """
        anomalies = []
        
        for metric_name, value in current_metrics.items():
            if metric_name == 'collection_timestamp':
                continue
            
            try:
                numeric_value = float(value)
            except (ValueError, TypeError):
                continue
            
            # Check against baseline
            if metric_name in self.baselines:
                baseline = self.baselines[metric_name]
                
                if baseline.std_dev > 0 and baseline.sample_count > 10:
                    # Z-score based anomaly detection
                    z_score = abs(numeric_value - baseline.mean) / baseline.std_dev
                    
                    if z_score > 3.0:  # 3-sigma rule
                        anomalies.append({
                            'type': 'statistical_anomaly',
                            'metric_name': metric_name,
                            'current_value': numeric_value,
                            'baseline_mean': baseline.mean,
                            'baseline_std': baseline.std_dev,
                            'z_score': z_score,
                            'detection_method': 'z_score',
                            'timestamp': datetime.utcnow()
                        })
        
        return anomalies
    
    async def _prepare_ml_features(self) -> Optional[np.ndarray]:
        """
        Prepare feature matrix for machine learning analysis.
        
        Returns:
            Feature matrix or None if insufficient data
        """
        if not ML_AVAILABLE:
            return None
        
        # Collect features from multiple metrics
        feature_names = []
        feature_data = []
        
        # Get metrics with sufficient history
        valid_metrics = {name: history for name, history in self.metrics_history.items() 
                        if len(history) >= 50}
        
        if len(valid_metrics) < 2:
            return None
        
        # Align timestamps and create feature vectors
        min_length = min(len(history) for history in valid_metrics.values())
        aligned_data = {}
        
        for metric_name, history in valid_metrics.items():
            # Take the last min_length points
            recent_data = list(history)[-min_length:]
            values = [value for timestamp, value in recent_data]
            aligned_data[metric_name] = values
        
        # Create feature matrix
        feature_matrix = []
        for i in range(min_length):
            feature_vector = []
            for metric_name in sorted(aligned_data.keys()):
                feature_vector.append(aligned_data[metric_name][i])
            feature_matrix.append(feature_vector)
        
        return np.array(feature_matrix)
    
    async def _extract_current_features(self, current_metrics: Dict[str, Any]) -> Optional[np.ndarray]:
        """
        Extract current feature vector for anomaly detection.
        
        Args:
            current_metrics: Current metric values
            
        Returns:
            Feature vector or None if insufficient data
        """
        features = []
        
        # Use same metric ordering as training data
        for metric_name in sorted(current_metrics.keys()):
            if metric_name == 'collection_timestamp':
                continue
            
            try:
                value = float(current_metrics[metric_name])
                features.append(value)
            except (ValueError, TypeError):
                continue
        
        return np.array(features) if features else None
    
    # =========================================================================
    # PREDICTIVE ALERT GENERATION
    # =========================================================================
    
    async def _generate_predictive_alerts(self, current_metrics: Dict[str, Any],
                                         anomalies: List[Dict[str, Any]]) -> List[PredictiveAlert]:
        """
        Generate predictive alerts based on analysis results.
        
        Args:
            current_metrics: Current metric values
            anomalies: Detected anomalies
            
        Returns:
            List of predictive alerts
        """
        predictions = []
        
        # Analyze trends for predictive issues
        for metric_name, trend in self.trend_analyses.items():
            prediction = await self._analyze_trend_for_prediction(metric_name, trend, current_metrics)
            if prediction:
                predictions.append(prediction)
        
        # Analyze anomalies for predictive insights
        for anomaly in anomalies:
            prediction = await self._analyze_anomaly_for_prediction(anomaly, current_metrics)
            if prediction:
                predictions.append(prediction)
        
        # Cross-correlation analysis
        correlation_predictions = await self._perform_correlation_analysis(current_metrics)
        predictions.extend(correlation_predictions)
        
        # Filter and deduplicate predictions
        filtered_predictions = await self._filter_predictions(predictions)
        
        # Update active predictions
        for prediction in filtered_predictions:
            self.active_predictions[prediction.prediction_id] = prediction
        
        return filtered_predictions
    
    async def _analyze_trend_for_prediction(self, metric_name: str, 
                                          trend: TrendAnalysis,
                                          current_metrics: Dict[str, Any]) -> Optional[PredictiveAlert]:
        """
        Analyze a trend to generate predictive alerts.
        
        Args:
            metric_name: Name of the metric
            trend: Trend analysis results
            current_metrics: Current metric values
            
        Returns:
            PredictiveAlert or None
        """
        # Memory leak detection
        if 'memory' in metric_name.lower() or 'heap' in metric_name.lower():
            if (trend.trend_direction == "increasing" and 
                trend.trend_strength > 0.7 and 
                trend.rate_of_change > 0):
                
                # Estimate time to critical threshold (e.g., 90% memory usage)
                current_value = trend.time_series_data[-1][1] if trend.time_series_data else 0
                critical_threshold = 90.0  # 90% memory usage
                
                if current_value < critical_threshold and trend.rate_of_change > 0:
                    time_to_critical = (critical_threshold - current_value) / trend.rate_of_change
                    predicted_time = datetime.utcnow() + timedelta(seconds=time_to_critical)
                    
                    confidence = PredictionConfidence.HIGH if trend.trend_strength > 0.8 else PredictionConfidence.MEDIUM
                    
                    return PredictiveAlert(
                        prediction_id=f"memory_leak_{metric_name}_{int(time.time())}",
                        issue_type=IssueType.MEMORY_LEAK,
                        confidence=confidence,
                        predicted_time=predicted_time,
                        current_severity=AlertSeverity.MEDIUM,
                        predicted_severity=AlertSeverity.CRITICAL,
                        description=f"Memory leak detected in {metric_name}. "
                                   f"Current usage: {current_value:.1f}%, "
                                   f"predicted to reach critical level in {time_to_critical/3600:.1f} hours",
                        affected_components=[metric_name.split('_')[0] if '_' in metric_name else 'application'],
                        root_cause_indicators=[
                            f"Steadily increasing {metric_name}",
                            f"Rate of change: {trend.rate_of_change:.3f} per second",
                            f"Trend strength: {trend.trend_strength:.2f}"
                        ],
                        supporting_evidence={
                            'trend_direction': trend.trend_direction,
                            'trend_strength': trend.trend_strength,
                            'rate_of_change': trend.rate_of_change,
                            'current_value': current_value,
                            'predicted_threshold_breach': critical_threshold
                        },
                        trend_analysis={
                            'rate_per_hour': trend.rate_of_change * 3600,
                            'acceleration': trend.acceleration
                        },
                        baseline_deviation=trend.anomaly_score,
                        preventive_actions=[
                            "Analyze heap dump for memory leaks",
                            "Review application code for memory allocation patterns",
                            "Consider increasing heap size temporarily",
                            "Enable verbose GC logging for analysis"
                        ],
                        monitoring_actions=[
                            "Increase memory monitoring frequency",
                            "Set up heap dump collection on threshold breach",
                            "Monitor GC frequency and duration"
                        ],
                        escalation_timeline={
                            "immediate": "Development team notification",
                            "1_hour": "Senior developer escalation",
                            "critical_threshold": "Emergency response team"
                        },
                        model_used="trend_analysis",
                        data_sources=[metric_name],
                        false_positive_probability=0.1 if confidence == PredictionConfidence.HIGH else 0.2
                    )
        
        # Performance degradation detection
        if ('response_time' in metric_name.lower() or 
            'latency' in metric_name.lower() or 
            'duration' in metric_name.lower()):
            
            if (trend.trend_direction == "increasing" and 
                trend.trend_strength > 0.6 and 
                trend.rate_of_change > 0):
                
                current_value = trend.time_series_data[-1][1] if trend.time_series_data else 0
                baseline_value = trend.mean_value
                
                if current_value > baseline_value * 1.5:  # 50% increase from baseline
                    confidence = PredictionConfidence.MEDIUM if trend.trend_strength > 0.7 else PredictionConfidence.LOW
                    
                    return PredictiveAlert(
                        prediction_id=f"performance_degradation_{metric_name}_{int(time.time())}",
                        issue_type=IssueType.PERFORMANCE_DEGRADATION,
                        confidence=confidence,
                        predicted_time=datetime.utcnow() + timedelta(hours=1),
                        current_severity=AlertSeverity.MEDIUM,
                        predicted_severity=AlertSeverity.HIGH,
                        description=f"Performance degradation detected in {metric_name}. "
                                   f"Current value: {current_value:.2f}, "
                                   f"baseline: {baseline_value:.2f}",
                        affected_components=['application_performance'],
                        root_cause_indicators=[
                            f"Increasing {metric_name}",
                            f"Current value {((current_value/baseline_value - 1) * 100):.1f}% above baseline",
                            f"Trend strength: {trend.trend_strength:.2f}"
                        ],
                        supporting_evidence={
                            'current_value': current_value,
                            'baseline_value': baseline_value,
                            'degradation_percent': (current_value/baseline_value - 1) * 100,
                            'trend_strength': trend.trend_strength
                        },
                        trend_analysis={
                            'rate_of_degradation': trend.rate_of_change
                        },
                        baseline_deviation=trend.anomaly_score,
                        preventive_actions=[
                            "Analyze application performance bottlenecks",
                            "Review recent code changes",
                            "Check database query performance",
                            "Monitor system resource usage"
                        ],
                        monitoring_actions=[
                            "Increase performance monitoring frequency",
                            "Enable detailed transaction tracing",
                            "Monitor related system metrics"
                        ],
                        escalation_timeline={
                            "30_minutes": "Performance team notification",
                            "2_hours": "Engineering manager escalation"
                        },
                        model_used="trend_analysis",
                        data_sources=[metric_name],
                        false_positive_probability=0.2
                    )
        
        return None
    
    async def _analyze_anomaly_for_prediction(self, anomaly: Dict[str, Any],
                                            current_metrics: Dict[str, Any]) -> Optional[PredictiveAlert]:
        """
        Analyze an anomaly to generate predictive insights.
        
        Args:
            anomaly: Detected anomaly
            current_metrics: Current metric values
            
        Returns:
            PredictiveAlert or None
        """
        if anomaly.get('type') == 'statistical_anomaly':
            metric_name = anomaly.get('metric_name')
            z_score = anomaly.get('z_score', 0)
            
            if z_score > 4.0:  # Very high anomaly
                confidence = PredictionConfidence.HIGH
                predicted_severity = AlertSeverity.CRITICAL
            elif z_score > 3.5:
                confidence = PredictionConfidence.MEDIUM
                predicted_severity = AlertSeverity.HIGH
            else:
                confidence = PredictionConfidence.LOW
                predicted_severity = AlertSeverity.MEDIUM
            
            return PredictiveAlert(
                prediction_id=f"anomaly_prediction_{metric_name}_{int(time.time())}",
                issue_type=IssueType.SECURITY_ANOMALY if 'security' in metric_name.lower() else IssueType.PERFORMANCE_DEGRADATION,
                confidence=confidence,
                predicted_time=datetime.utcnow() + timedelta(minutes=15),
                current_severity=AlertSeverity.MEDIUM,
                predicted_severity=predicted_severity,
                description=f"Statistical anomaly detected in {metric_name}. "
                           f"Z-score: {z_score:.2f}, "
                           f"Current value: {anomaly.get('current_value', 'unknown')}",
                affected_components=[metric_name.split('_')[0] if '_' in metric_name else 'system'],
                root_cause_indicators=[
                    f"Statistical anomaly in {metric_name}",
                    f"Z-score: {z_score:.2f} (threshold: 3.0)",
                    f"Deviation from baseline: {anomaly.get('current_value', 0) - anomaly.get('baseline_mean', 0):.2f}"
                ],
                supporting_evidence=anomaly,
                trend_analysis={},
                baseline_deviation=z_score / 3.0,  # Normalize to 0-1
                preventive_actions=[
                    f"Investigate cause of anomaly in {metric_name}",
                    "Review recent system changes",
                    "Check for external factors affecting the metric"
                ],
                monitoring_actions=[
                    "Increase monitoring frequency for related metrics",
                    "Set up additional alerting thresholds"
                ],
                escalation_timeline={
                    "immediate": "Operations team notification"
                },
                model_used="statistical_analysis",
                data_sources=[metric_name],
                false_positive_probability=0.15
            )
        
        return None
    
    async def _perform_correlation_analysis(self, current_metrics: Dict[str, Any]) -> List[PredictiveAlert]:
        """
        Perform cross-correlation analysis to detect systemic issues.
        
        Args:
            current_metrics: Current metric values
            
        Returns:
            List of correlation-based predictive alerts
        """
        predictions = []
        
        # Simplified correlation analysis
        # Look for patterns where multiple metrics are trending in concerning directions
        
        concerning_trends = []
        for metric_name, trend in self.trend_analyses.items():
            if (trend.trend_direction in ["increasing", "decreasing"] and 
                trend.trend_strength > 0.6 and 
                trend.anomaly_score > 0.5):
                concerning_trends.append((metric_name, trend))
        
        # If multiple concerning trends, generate systemic alert
        if len(concerning_trends) >= 3:
            prediction = PredictiveAlert(
                prediction_id=f"systemic_issue_{int(time.time())}",
                issue_type=IssueType.CONFIGURATION_DRIFT,
                confidence=PredictionConfidence.MEDIUM,
                predicted_time=datetime.utcnow() + timedelta(hours=1),
                current_severity=AlertSeverity.MEDIUM,
                predicted_severity=AlertSeverity.HIGH,
                description=f"Systemic issues detected across {len(concerning_trends)} metrics. "
                           f"This may indicate a broader system problem.",
                affected_components=['system_wide'],
                root_cause_indicators=[
                    f"Multiple metrics showing concerning trends: {', '.join([name for name, _ in concerning_trends])}",
                    "Possible systemic issue or configuration drift"
                ],
                supporting_evidence={
                    'concerning_metrics': [name for name, _ in concerning_trends],
                    'trend_count': len(concerning_trends)
                },
                trend_analysis={},
                baseline_deviation=sum(trend.anomaly_score for _, trend in concerning_trends) / len(concerning_trends),
                preventive_actions=[
                    "Review recent system-wide changes",
                    "Check infrastructure health",
                    "Analyze cross-metric correlations",
                    "Review configuration drift"
                ],
                monitoring_actions=[
                    "Increase monitoring across all affected metrics",
                    "Enable detailed system-level logging"
                ],
                escalation_timeline={
                    "30_minutes": "System administrator notification",
                    "1_hour": "Architecture team escalation"
                },
                model_used="correlation_analysis",
                data_sources=[name for name, _ in concerning_trends],
                false_positive_probability=0.25
            )
            predictions.append(prediction)
        
        return predictions
    
    async def _filter_predictions(self, predictions: List[PredictiveAlert]) -> List[PredictiveAlert]:
        """
        Filter and deduplicate predictions based on confidence and relevance.
        
        Args:
            predictions: Raw predictions to filter
            
        Returns:
            Filtered list of predictions
        """
        filtered = []
        
        for prediction in predictions:
            # Filter by minimum confidence
            if prediction.confidence in [PredictionConfidence.UNCERTAIN]:
                continue
            
            # Check for duplicates
            is_duplicate = False
            for existing in filtered:
                if (existing.issue_type == prediction.issue_type and 
                    existing.affected_components == prediction.affected_components):
                    # Keep the one with higher confidence
                    if prediction.confidence.value > existing.confidence.value:
                        filtered.remove(existing)
                        break
                    else:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                filtered.append(prediction)
        
        return filtered
    
    # =========================================================================
    # MACHINE LEARNING MODEL MANAGEMENT
    # =========================================================================
    
    async def _initialize_ml_models(self) -> None:
        """
        Initialize machine learning models for anomaly detection.
        """
        if not ML_AVAILABLE:
            logger.warning("ML libraries not available, skipping ML model initialization")
            return
        
        try:
            # Initialize isolation forest for anomaly detection
            self.ml_models['isolation_forest'] = IsolationForest(
                contamination=0.1,
                random_state=42,
                n_estimators=100
            )
            
            # Initialize scaler for feature normalization
            self.scalers['standard'] = StandardScaler()
            
            logger.info("ML models initialized successfully", agent_id=self.agent_id)
            
        except Exception as e:
            logger.error("Error initializing ML models", 
                        agent_id=self.agent_id, error=str(e))
    
    async def _update_ml_models(self) -> int:
        """
        Update and retrain ML models with recent data.
        
        Returns:
            Number of models updated
        """
        if not ML_AVAILABLE:
            return 0
        
        updated_count = 0
        
        try:
            # Check if enough new data for retraining
            feature_data = await self._prepare_ml_features()
            
            if feature_data is not None and len(feature_data) > 100:
                # Retrain isolation forest
                if 'isolation_forest' in self.ml_models:
                    self.ml_models['isolation_forest'].fit(feature_data)
                    self.model_last_trained['isolation_forest'] = datetime.utcnow()
                    updated_count += 1
                    
                    logger.info("Isolation forest model retrained",
                               agent_id=self.agent_id,
                               data_points=len(feature_data))
        
        except Exception as e:
            logger.error("Error updating ML models",
                        agent_id=self.agent_id, error=str(e))
        
        return updated_count
    
    # =========================================================================
    # DATA PERSISTENCE AND CLEANUP
    # =========================================================================
    
    async def _load_saved_data(self) -> None:
        """
        Load saved baselines and models from disk.
        """
        try:
            # Load baselines
            baselines_file = self.data_dir / 'baselines.pickle'
            if baselines_file.exists():
                with open(baselines_file, 'rb') as f:
                    self.baselines = pickle.load(f)
                logger.info("Loaded saved baselines", 
                           agent_id=self.agent_id, 
                           baseline_count=len(self.baselines))
            
            # Load ML models
            models_file = self.data_dir / 'ml_models.pickle'
            if models_file.exists() and ML_AVAILABLE:
                with open(models_file, 'rb') as f:
                    self.ml_models = pickle.load(f)
                logger.info("Loaded saved ML models",
                           agent_id=self.agent_id,
                           model_count=len(self.ml_models))
        
        except Exception as e:
            logger.error("Error loading saved data",
                        agent_id=self.agent_id, error=str(e))
    
    async def _save_current_data(self) -> None:
        """
        Save current baselines and models to disk.
        """
        try:
            # Save baselines
            baselines_file = self.data_dir / 'baselines.pickle'
            with open(baselines_file, 'wb') as f:
                pickle.dump(self.baselines, f)
            
            # Save ML models
            if self.ml_models and ML_AVAILABLE:
                models_file = self.data_dir / 'ml_models.pickle'
                with open(models_file, 'wb') as f:
                    pickle.dump(self.ml_models, f)
            
            logger.info("Saved current data",
                       agent_id=self.agent_id)
        
        except Exception as e:
            logger.error("Error saving current data",
                        agent_id=self.agent_id, error=str(e))
    
    async def _cleanup_old_data(self) -> None:
        """
        Clean up old predictions and historical data.
        """
        try:
            current_time = datetime.utcnow()
            
            # Remove old predictions
            expired_predictions = []
            for pred_id, prediction in self.active_predictions.items():
                if current_time - prediction.last_updated > timedelta(hours=24):
                    expired_predictions.append(pred_id)
            
            for pred_id in expired_predictions:
                del self.active_predictions[pred_id]
            
            # Limit historical data size
            for metric_name, history in self.metrics_history.items():
                if len(history) > 10000:
                    # Keep only recent data
                    cutoff_time = current_time - timedelta(days=7)
                    filtered_history = deque(
                        [(ts, value) for ts, value in history if ts > cutoff_time],
                        maxlen=10000
                    )
                    self.metrics_history[metric_name] = filtered_history
            
            logger.debug("Cleaned up old data",
                        agent_id=self.agent_id,
                        expired_predictions=len(expired_predictions))
        
        except Exception as e:
            logger.error("Error during data cleanup",
                        agent_id=self.agent_id, error=str(e))
    
    async def _validate_configuration(self) -> None:
        """
        Validate agent configuration for required parameters.
        """
        required_params = ['prediction_window_hours', 'baseline_period_days']
        
        for param in required_params:
            if param not in self.config:
                logger.warning("Missing required configuration parameter",
                              agent_id=self.agent_id, parameter=param)
        
        # Validate numeric ranges
        if self.prediction_window < 1 or self.prediction_window > 24:
            logger.warning("Invalid prediction window", 
                          agent_id=self.agent_id,
                          value=self.prediction_window)
        
        if self.baseline_period_days < 1 or self.baseline_period_days > 30:
            logger.warning("Invalid baseline period",
                          agent_id=self.agent_id,
                          value=self.baseline_period_days)
    
    async def _emit_results(self, predictions: List[PredictiveAlert],
                           anomalies: List[Dict[str, Any]],
                           current_metrics: Dict[str, Any]) -> None:
        """
        Emit alerts and metrics based on analysis results.
        
        Args:
            predictions: Generated predictive alerts
            anomalies: Detected anomalies
            current_metrics: Current metric values
        """
        # Emit predictive alerts
        for prediction in predictions:
            alert = Alert(
                id=prediction.prediction_id,
                title=f"Predictive Alert: {prediction.issue_type.value}",
                description=prediction.description,
                severity=prediction.current_severity,
                source=self.agent_id,
                timestamp=datetime.utcnow(),
                tags=['predictive', 'proactive', prediction.issue_type.value],
                metadata={
                    'prediction': prediction,
                    'confidence': prediction.confidence.value,
                    'predicted_time': prediction.predicted_time.isoformat(),
                    'affected_components': prediction.affected_components
                }
            )
            await self.emit_alert(alert)
        
        # Emit anomaly alerts
        for anomaly in anomalies:
            alert = Alert(
                id=f"anomaly_{int(time.time())}_{anomaly.get('metric_name', 'unknown')}",
                title=f"Anomaly Detected: {anomaly.get('metric_name', 'Unknown Metric')}",
                description=f"Anomaly detected using {anomaly.get('detection_method', 'unknown')} method",
                severity=AlertSeverity.MEDIUM,
                source=self.agent_id,
                timestamp=datetime.utcnow(),
                tags=['anomaly', 'detection'],
                metadata=anomaly
            )
            await self.emit_alert(alert)
        
        # Emit performance metrics
        await self.emit_metric(MonitoringMetric(
            name="proactive_detection_predictions_generated",
            value=len(predictions),
            timestamp=datetime.utcnow(),
            tags={'agent': self.agent_id}
        ))
        
        await self.emit_metric(MonitoringMetric(
            name="proactive_detection_anomalies_detected",
            value=len(anomalies),
            timestamp=datetime.utcnow(),
            tags={'agent': self.agent_id}
        ))
        
        await self.emit_metric(MonitoringMetric(
            name="proactive_detection_active_predictions",
            value=len(self.active_predictions),
            timestamp=datetime.utcnow(),
            tags={'agent': self.agent_id}
        ))


# =============================================================================
# AGENT FACTORY REGISTRATION
# =============================================================================

@AgentFactory.register('proactive_detection')
class ProactiveDetectionAgentFactory:
    """Factory for creating ProactiveDetectionAgent instances."""
    
    @staticmethod
    def create_agent(agent_id: str, config: Dict[str, Any]) -> ProactiveDetectionAgent:
        """
        Create a new ProactiveDetectionAgent instance.
        
        Args:
            agent_id: Unique identifier for the agent
            config: Configuration dictionary
            
        Returns:
            Configured ProactiveDetectionAgent instance
        """
        return ProactiveDetectionAgent(agent_id, config)
    
    @staticmethod
    def get_default_config() -> Dict[str, Any]:
        """
        Get default configuration for ProactiveDetectionAgent.
        
        Returns:
            Default configuration dictionary
        """
        return {
            'prediction_window_hours': 2,
            'baseline_period_days': 7,
            'anomaly_threshold': 0.7,
            'min_confidence_threshold': 0.6,
            'enable_ml_detection': True,
            'data_directory': '/var/lib/crown-jewel/proactive-detection'
        }